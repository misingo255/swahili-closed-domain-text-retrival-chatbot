{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60054847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#please run this once\n",
    "#installing nltk\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1201be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries and modules\n",
    "import io\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a65df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#please run this once\n",
    "#installing nltk modules and libraries\n",
    "#nltk.download('popular', quiet=True)\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "586df303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching data from the data source/file\n",
    "file = open('tanakishi.txt','r',errors = 'ignore')\n",
    "raw = file.read()\n",
    "raw = raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e06632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the data\n",
    "sent_tokens = nltk.sent_tokenize(raw)\n",
    "word_tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4e8e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for lametizing the data tokens\n",
    "def LemetizeTokens(tokens):\n",
    "    lemmer = nltk.stem.WordNetLemmatizer()\n",
    "    return [lemmer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aaaa375",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for normalizing the lametized data tokens\n",
    "def LemetizationNormalize(text):\n",
    "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "    return LemetizeTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3134496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the user inputs and bot responces\n",
    "USER_GREETING_INPUTS = (\"habari\", \"habari za sahizi\", \"habari za asubuhi\", \"habari za mchana\", \"habari za jioni\", \"habari za usiku\", \"mambo\", \"habari?\", \"habari za sahizi?\", \"habari za asubuhi?\", \"habari za mchana?\", \"habari za jioni?\", \"habari za usiku?\", \"mambo?\")\n",
    "BOT_GREETING_RESPONSES = [\"nzuri\", \"poa\", \"salama\", \"kheri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7710e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frunction for processing the user greeting  and bot responce to that greeting\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in USER_GREETING_INPUTS:\n",
    "            return random.choice(BOT_GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f2b75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading stop words\n",
    "data_file = pd.read_csv('Common Swahili Stop-words.csv')\n",
    "swahili_stop_words = list(data_file['StopWords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86c1e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for processing bot responses\n",
    "def response(user_response):\n",
    "    bot_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemetizationNormalize, stop_words=swahili_stop_words)\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=bot_response+\"Samahani sijakuelewa, unaweza rudia tena!!\"\n",
    "        return bot_response\n",
    "    else:\n",
    "        bot_response = bot_response+sent_tokens[idx]\n",
    "        return bot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6dba9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: Habari, tuzungumze kuhusu tanakishi/kompyuta, iwapo huitaji kuendelea na mazungumzo, sema inatosha\n",
      "YOU: mambo?\n",
      "BOT: poa\n",
      "YOU: inatosha\n",
      "BOT: asante, karibu tena!!\n"
     ]
    }
   ],
   "source": [
    "#conversation processing\n",
    "flag=True\n",
    "print(\"BOT: Habari, tuzungumze kuhusu tanakishi/kompyuta, iwapo huitaji kuendelea na mazungumzo, sema inatosha\")\n",
    "while(flag==True):\n",
    "    user_response = input(\"YOU: \")\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='inatosha'):\n",
    "        if(user_response=='asante' or user_response=='asante pia' ):\n",
    "            flag=False\n",
    "            print(\"BOT: usijali\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"BOT: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"BOT: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"BOT: asante, karibu tena!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad52fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06503e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "24d825a476dd4e09491c3f07fb6e89747055561f003ff5cb7538ec1c82fcdd1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
